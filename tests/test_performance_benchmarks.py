"""
æ€§èƒ½åŸºæº–æ¸¬è©¦æ¨¡çµ„
æ¸¬è©¦å„å€‹çµ„ä»¶çš„æ€§èƒ½è¡¨ç¾ä¸¦å»ºç«‹åŸºæº–ç·šï¼Œç”¨æ–¼æª¢æ¸¬æ€§èƒ½å›æ­¸
"""

import os
import statistics
import tempfile
import time
from pathlib import Path
from typing import Callable, Dict, List

import psutil
import pytest

from src.core.csv_generator import CSVGenerator
from src.core.essay_question_parser import EssayQuestionParser
from src.core.mixed_format_parser import MixedFormatParser

# å°å…¥è¦æ¸¬è©¦çš„æ¨¡çµ„
from src.core.pdf_processor import PDFProcessor
from src.core.question_parser import QuestionParser
from src.core.ultimate_question_parser import UltimateQuestionParser
from src.utils.concurrent_processor import ConcurrentProcessor, ProcessingTask


class PerformanceBenchmark:
    """æ€§èƒ½åŸºæº–æ¸¬è©¦é¡"""

    def __init__(self):
        self.results: Dict[str, Dict] = {}
        self.process = psutil.Process(os.getpid())

    def measure_time(self, func: Callable, *args, **kwargs) -> tuple:
        """æ¸¬é‡å‡½æ•¸åŸ·è¡Œæ™‚é–“å’Œè¨˜æ†¶é«”ä½¿ç”¨"""
        # è¨˜éŒ„é–‹å§‹ç‹€æ…‹
        start_time = time.perf_counter()
        start_memory = self.process.memory_info().rss / 1024 / 1024  # MB

        # åŸ·è¡Œå‡½æ•¸
        result = func(*args, **kwargs)

        # è¨˜éŒ„çµæŸç‹€æ…‹
        end_time = time.perf_counter()
        end_memory = self.process.memory_info().rss / 1024 / 1024  # MB

        execution_time = end_time - start_time
        memory_used = end_memory - start_memory

        return result, execution_time, memory_used

    def run_multiple_times(self, func: Callable, iterations: int = 5, *args, **kwargs) -> Dict:
        """å¤šæ¬¡é‹è¡Œæ¸¬è©¦ä»¥ç²å¾—å¯é çš„çµæœ"""
        times = []
        memories = []

        for _ in range(iterations):
            _, exec_time, memory_used = self.measure_time(func, *args, **kwargs)
            times.append(exec_time)
            memories.append(memory_used)

        return {
            "avg_time": statistics.mean(times),
            "min_time": min(times),
            "max_time": max(times),
            "std_time": statistics.stdev(times) if len(times) > 1 else 0,
            "avg_memory": statistics.mean(memories),
            "min_memory": min(memories),
            "max_memory": max(memories),
        }


@pytest.fixture
def benchmark():
    """æä¾›æ€§èƒ½åŸºæº–æ¸¬è©¦å·¥å…·"""
    return PerformanceBenchmark()


@pytest.fixture
def sample_pdf_content():
    """æä¾›æ¸¬è©¦ç”¨çš„ PDF å…§å®¹"""
    return (
        """
    1. ä¸‹åˆ—ä½•è€…æ­£ç¢ºï¼Ÿ
    (A) é¸é … A
    (B) é¸é … B
    (C) é¸é … C
    (D) é¸é … D

    2. ä»¥ä¸‹æ•˜è¿°ä½•è€…ç‚ºçœŸï¼Ÿ
    (A) ç¬¬ä¸€å€‹é¸é …
    (B) ç¬¬äºŒå€‹é¸é …
    (C) ç¬¬ä¸‰å€‹é¸é …
    (D) ç¬¬å››å€‹é¸é …
    """
        * 10
    )  # é‡è¤‡10æ¬¡ä»¥æ¨¡æ“¬è¼ƒå¤§çš„å…§å®¹


@pytest.fixture
def large_pdf_content():
    """æä¾›å¤§å‹ PDF æ¸¬è©¦å…§å®¹"""
    base_question = """
    {}. æ¸¬è©¦é¡Œç›®å…§å®¹ï¼Ÿ
    (A) é¸é … A çš„å…§å®¹æè¿°
    (B) é¸é … B çš„å…§å®¹æè¿°
    (C) é¸é … C çš„å…§å®¹æè¿°
    (D) é¸é … D çš„å…§å®¹æè¿°

    """
    return "".join([base_question.format(i) for i in range(1, 101)])  # 100 é¡Œ


class TestPDFProcessorPerformance:
    """æ¸¬è©¦ PDF è™•ç†å™¨çš„æ€§èƒ½"""

    def test_pdf_text_extraction_speed(self, benchmark, tmp_path):
        """æ¸¬è©¦ PDF æ–‡å­—æå–é€Ÿåº¦

        åŸºæº–ï¼š< 1 ç§’/é ï¼ˆç´”æ–‡å­— PDFï¼‰
        """
        # å‰µå»ºæ¸¬è©¦ PDFï¼ˆå¯¦éš›æ¸¬è©¦ä¸­æ‡‰ä½¿ç”¨çœŸå¯¦ PDFï¼‰
        # é€™è£¡ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
        processor = PDFProcessor()

        # æ¨¡æ“¬è™•ç†
        def extract_text():
            # å¯¦éš›æ¸¬è©¦ä¸­æ‡‰è©²è™•ç†çœŸå¯¦ PDF
            return "æ¸¬è©¦æ–‡å­—" * 1000

        results = benchmark.run_multiple_times(extract_text, iterations=10)

        # æ–·è¨€æ€§èƒ½åŸºæº–
        assert results["avg_time"] < 1.0, f"PDF æ–‡å­—æå–å¤ªæ…¢: {results['avg_time']:.3f}s"
        assert results["avg_memory"] < 50, f"è¨˜æ†¶é«”ä½¿ç”¨éå¤š: {results['avg_memory']:.2f}MB"

        print(f"\nğŸ“Š PDF æ–‡å­—æå–æ€§èƒ½:")
        print(f"  å¹³å‡æ™‚é–“: {results['avg_time']:.3f}s")
        print(f"  è¨˜æ†¶é«”ä½¿ç”¨: {results['avg_memory']:.2f}MB")


class TestQuestionParserPerformance:
    """æ¸¬è©¦é¡Œç›®è§£æå™¨çš„æ€§èƒ½"""

    def test_standard_parser_speed(self, benchmark, sample_pdf_content):
        """æ¸¬è©¦æ¨™æº–è§£æå™¨é€Ÿåº¦

        åŸºæº–ï¼š< 0.1 ç§’/10é¡Œ
        """
        parser = QuestionParser()

        def parse_questions():
            return parser.parse_questions(sample_pdf_content)

        results = benchmark.run_multiple_times(parse_questions, iterations=10)

        # æ–·è¨€æ€§èƒ½åŸºæº–
        assert results["avg_time"] < 0.1, f"è§£æé€Ÿåº¦å¤ªæ…¢: {results['avg_time']:.3f}s"
        assert results["avg_memory"] < 10, f"è¨˜æ†¶é«”ä½¿ç”¨éå¤š: {results['avg_memory']:.2f}MB"

        print(f"\nğŸ“Š æ¨™æº–è§£æå™¨æ€§èƒ½:")
        print(f"  å¹³å‡æ™‚é–“: {results['avg_time']:.3f}s")
        print(f"  æ¨™æº–å·®: {results['std_time']:.3f}s")
        print(f"  è¨˜æ†¶é«”: {results['avg_memory']:.2f}MB")

    def test_large_document_parsing(self, benchmark, large_pdf_content):
        """æ¸¬è©¦å¤§æ–‡ä»¶è§£ææ€§èƒ½

        åŸºæº–ï¼š< 1 ç§’/100é¡Œ
        """
        parser = QuestionParser()

        def parse_large():
            return parser.parse_questions(large_pdf_content)

        results = benchmark.run_multiple_times(parse_large, iterations=5)

        assert results["avg_time"] < 1.0, f"å¤§æ–‡ä»¶è§£æå¤ªæ…¢: {results['avg_time']:.3f}s"

        print(f"\nğŸ“Š å¤§æ–‡ä»¶è§£ææ€§èƒ½ (100é¡Œ):")
        print(f"  å¹³å‡æ™‚é–“: {results['avg_time']:.3f}s")
        print(f"  ååé‡: {100 / results['avg_time']:.1f} é¡Œ/ç§’")


class TestParserComparison:
    """æ¯”è¼ƒä¸åŒè§£æå™¨çš„æ€§èƒ½"""

    @pytest.mark.parametrize(
        "parser_class,name",
        [
            (QuestionParser, "æ¨™æº–è§£æå™¨"),
            (EssayQuestionParser, "ç”³è«–é¡Œè§£æå™¨"),
            (MixedFormatParser, "æ··åˆæ ¼å¼è§£æå™¨"),
            (UltimateQuestionParser, "çµ‚æ¥µè§£æå™¨"),
        ],
    )
    def test_parser_comparison(self, benchmark, sample_pdf_content, parser_class, name):
        """æ¯”è¼ƒä¸åŒè§£æå™¨çš„æ€§èƒ½"""
        parser = parser_class()

        def parse():
            try:
                return parser.parse(sample_pdf_content)
            except Exception:
                return []

        results = benchmark.run_multiple_times(parse, iterations=5)

        print(f"\nğŸ“Š {name} æ€§èƒ½:")
        print(f"  å¹³å‡æ™‚é–“: {results['avg_time']:.3f}s")
        print(f"  è¨˜æ†¶é«”: {results['avg_memory']:.2f}MB")

        # æ¯å€‹è§£æå™¨éƒ½æ‡‰è©²åœ¨åˆç†æ™‚é–“å…§å®Œæˆ
        assert results["avg_time"] < 2.0, f"{name} æ€§èƒ½ä¸é”æ¨™"


class TestConcurrentProcessingPerformance:
    """æ¸¬è©¦ä¸¦ç™¼è™•ç†æ€§èƒ½"""

    def test_concurrent_speedup(self, benchmark, tmp_path):
        """æ¸¬è©¦ä¸¦ç™¼è™•ç†çš„é€Ÿåº¦æå‡

        é æœŸï¼š3-4 å€é€Ÿåº¦æå‡ï¼ˆ4 workerï¼‰
        """
        # å‰µå»ºæ¸¬è©¦æ–‡ä»¶
        test_files = []
        for i in range(4):
            test_file = tmp_path / f"test_{i}.txt"
            test_file.write_text(f"æ¸¬è©¦å…§å®¹ {i}" * 100)
            test_files.append(str(test_file))

        def process_file(file_path):
            """æ¨¡æ“¬è™•ç†å–®å€‹æ–‡ä»¶"""
            time.sleep(0.1)  # æ¨¡æ“¬è™•ç†æ™‚é–“
            with open(file_path, "r", encoding="utf-8") as f:
                return len(f.read())

        # æ¸¬è©¦é †åºè™•ç†
        def sequential_processing():
            return [process_file(f) for f in test_files]

        # æ¸¬è©¦ä¸¦ç™¼è™•ç†
        def concurrent_processing():
            processor = ConcurrentProcessor(max_workers=4)
            # Convert file paths to ProcessingTask objects with task_id
            tasks = [ProcessingTask(task_id=i, pdf_path=f, output_dir=str(tmp_path)) 
                    for i, f in enumerate(test_files)]
            
            # Create a wrapper function that accepts ProcessingTask
            def task_processor(task: ProcessingTask):
                return process_file(task.pdf_path)
            
            return processor.process_batch(tasks, task_processor)

        # æ¸¬é‡æ€§èƒ½
        _, seq_time, _ = benchmark.measure_time(sequential_processing)
        _, con_time, _ = benchmark.measure_time(concurrent_processing)

        speedup = seq_time / con_time

        print(f"\nğŸ“Š ä¸¦ç™¼è™•ç†æ€§èƒ½:")
        print(f"  é †åºè™•ç†: {seq_time:.3f}s")
        print(f"  ä¸¦ç™¼è™•ç†: {con_time:.3f}s")
        print(f"  é€Ÿåº¦æå‡: {speedup:.2f}x")

        # æ‡‰è©²æœ‰æ˜é¡¯çš„é€Ÿåº¦æå‡
        assert speedup > 2.0, f"ä¸¦ç™¼é€Ÿåº¦æå‡ä¸è¶³: {speedup:.2f}x"


class TestMemoryUsage:
    """æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨"""

    def test_memory_leak(self, benchmark, sample_pdf_content):
        """æ¸¬è©¦æ˜¯å¦å­˜åœ¨è¨˜æ†¶é«”æ´©æ¼"""
        parser = QuestionParser()

        initial_memory = benchmark.process.memory_info().rss / 1024 / 1024

        # å¤šæ¬¡åŸ·è¡Œ
        for _ in range(100):
            parser.parse_questions(sample_pdf_content)

        final_memory = benchmark.process.memory_info().rss / 1024 / 1024
        memory_increase = final_memory - initial_memory

        print(f"\nğŸ“Š è¨˜æ†¶é«”æ´©æ¼æ¸¬è©¦ (100 æ¬¡è¿­ä»£):")
        print(f"  åˆå§‹è¨˜æ†¶é«”: {initial_memory:.2f}MB")
        print(f"  æœ€çµ‚è¨˜æ†¶é«”: {final_memory:.2f}MB")
        print(f"  è¨˜æ†¶é«”å¢é•·: {memory_increase:.2f}MB")

        # è¨˜æ†¶é«”å¢é•·æ‡‰è©²åœ¨åˆç†ç¯„åœå…§ï¼ˆ< 50MBï¼‰
        assert memory_increase < 50, f"å¯èƒ½å­˜åœ¨è¨˜æ†¶é«”æ´©æ¼: {memory_increase:.2f}MB"

    def test_large_file_memory_efficiency(self, benchmark, large_pdf_content):
        """æ¸¬è©¦å¤§æ–‡ä»¶è™•ç†çš„è¨˜æ†¶é«”æ•ˆç‡

        åŸºæº–ï¼š< 100MB for 100 é¡Œ
        """
        parser = QuestionParser()

        _, exec_time, memory_used = benchmark.measure_time(parser.parse_questions, large_pdf_content)

        print(f"\nğŸ“Š å¤§æ–‡ä»¶è¨˜æ†¶é«”æ•ˆç‡ (100é¡Œ):")
        print(f"  åŸ·è¡Œæ™‚é–“: {exec_time:.3f}s")
        print(f"  è¨˜æ†¶é«”ä½¿ç”¨: {memory_used:.2f}MB")
        print(f"  è¨˜æ†¶é«”æ•ˆç‡: {memory_used / 100:.2f}MB/é¡Œ")

        assert memory_used < 100, f"è¨˜æ†¶é«”ä½¿ç”¨éå¤š: {memory_used:.2f}MB"


class TestCSVGeneratorPerformance:
    """æ¸¬è©¦ CSV ç”Ÿæˆå™¨æ€§èƒ½"""

    def test_csv_generation_speed(self, benchmark, tmp_path):
        """æ¸¬è©¦ CSV ç”Ÿæˆé€Ÿåº¦

        åŸºæº–ï¼š< 0.5 ç§’/100é¡Œ
        """
        generator = CSVGenerator()

        # å‰µå»ºæ¸¬è©¦æ•¸æ“š
        test_questions = [
            {
                "é¡Œè™Ÿ": i,
                "é¡Œç›®": f"æ¸¬è©¦é¡Œç›® {i}",
                "é¡Œå‹": "é¸æ“‡é¡Œ",
                "é¸é …A": f"é¸é … A {i}",
                "é¸é …B": f"é¸é … B {i}",
                "é¸é …C": f"é¸é … C {i}",
                "é¸é …D": f"é¸é … D {i}",
            }
            for i in range(1, 101)
        ]

        output_file = tmp_path / "test_output.csv"

        def generate_csv():
            # Use the correct method - generate_questions_csv
            answers = {str(i): "A" for i in range(1, 101)}
            generator.generate_questions_csv(test_questions, answers, str(output_file))

        results = benchmark.run_multiple_times(generate_csv, iterations=5)

        print(f"\nğŸ“Š CSV ç”Ÿæˆæ€§èƒ½ (100é¡Œ):")
        print(f"  å¹³å‡æ™‚é–“: {results['avg_time']:.3f}s")
        print(f"  ååé‡: {100 / results['avg_time']:.1f} é¡Œ/ç§’")

        assert results["avg_time"] < 0.5, f"CSV ç”Ÿæˆå¤ªæ…¢: {results['avg_time']:.3f}s"


class TestPerformanceRegression:
    """æ€§èƒ½å›æ­¸æ¸¬è©¦"""

    # å®šç¾©æ€§èƒ½åŸºæº–ç·šï¼ˆé€™äº›å€¼æ‡‰è©²åŸºæ–¼å¯¦éš›æ¸¬é‡çµæœè¨­å®šï¼‰
    BENCHMARKS = {
        "pdf_extraction": {"max_time": 1.0, "max_memory": 50},
        "question_parsing": {"max_time": 0.1, "max_memory": 10},
        "large_document": {"max_time": 1.0, "max_memory": 100},
        "csv_generation": {"max_time": 0.5, "max_memory": 20},
    }

    def test_no_performance_regression(self, benchmark, sample_pdf_content):
        """ç¢ºä¿æ²’æœ‰æ€§èƒ½å›æ­¸"""
        parser = QuestionParser()

        results = benchmark.run_multiple_times(parser.parse_questions, iterations=10, text=sample_pdf_content)

        baseline = self.BENCHMARKS["question_parsing"]

        # æª¢æŸ¥æ˜¯å¦ç¬¦åˆåŸºæº–
        time_regression = results["avg_time"] > baseline["max_time"]
        memory_regression = results["avg_memory"] > baseline["max_memory"]

        print(f"\nğŸ“Š æ€§èƒ½å›æ­¸æ¸¬è©¦:")
        print(f"  ç•¶å‰æ™‚é–“: {results['avg_time']:.3f}s (åŸºæº–: {baseline['max_time']}s)")
        print(f"  ç•¶å‰è¨˜æ†¶é«”: {results['avg_memory']:.2f}MB (åŸºæº–: {baseline['max_memory']}MB)")

        if time_regression:
            pytest.fail(f"âš ï¸  æª¢æ¸¬åˆ°æ™‚é–“æ€§èƒ½å›æ­¸: " f"{results['avg_time']:.3f}s > {baseline['max_time']}s")

        if memory_regression:
            pytest.fail(f"âš ï¸  æª¢æ¸¬åˆ°è¨˜æ†¶é«”æ€§èƒ½å›æ­¸: " f"{results['avg_memory']:.2f}MB > {baseline['max_memory']}MB")


class TestThroughput:
    """æ¸¬è©¦ç³»çµ±ååé‡"""

    def test_questions_per_second(self, benchmark, large_pdf_content):
        """æ¸¬è©¦æ¯ç§’è™•ç†é¡Œç›®æ•¸é‡

        åŸºæº–ï¼š> 100 é¡Œ/ç§’
        """
        parser = QuestionParser()

        _, exec_time, _ = benchmark.measure_time(parser.parse_questions, large_pdf_content)

        throughput = 100 / exec_time  # 100é¡Œçš„ååé‡

        print(f"\nğŸ“Š ç³»çµ±ååé‡:")
        print(f"  è™•ç†æ™‚é–“: {exec_time:.3f}s (100é¡Œ)")
        print(f"  ååé‡: {throughput:.1f} é¡Œ/ç§’")

        assert throughput > 100, f"ååé‡ä¸è¶³: {throughput:.1f} é¡Œ/ç§’"


# é‹è¡ŒåŸºæº–æ¸¬è©¦çš„ä¸»å‡½æ•¸
if __name__ == "__main__":
    pytest.main(
        [
            __file__,
            "-v",
            "--tb=short",
            "-s",  # é¡¯ç¤º print è¼¸å‡º
            "--benchmark-only",  # åƒ…é‹è¡ŒåŸºæº–æ¸¬è©¦
        ]
    )
