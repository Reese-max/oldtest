#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æº–ç¢ºç‡åˆ†æè…³æœ¬
åˆ†æé¡Œç›®æå–çš„æº–ç¢ºç‡å’Œè³ªé‡
"""

import os
import json
import csv
from typing import Dict, List, Any, Tuple
from pathlib import Path

class AccuracyAnalyzer:
    """æº–ç¢ºç‡åˆ†æå™¨"""
    
    def __init__(self, results_file: str):
        self.results_file = results_file
        self.results = self._load_results()
        self.analysis = {
            "overall_accuracy": 0.0,
            "category_accuracy": {},
            "subject_accuracy": {},
            "quality_metrics": {},
            "recommendations": []
        }
    
    def _load_results(self) -> Dict[str, Any]:
        """è¼‰å…¥æ¸¬è©¦çµæœ"""
        with open(self.results_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def analyze_overall_accuracy(self) -> float:
        """åˆ†ææ•´é«”æº–ç¢ºç‡"""
        total_subjects = self.results["test_info"]["total_subjects"]
        successful = self.results["test_info"]["successful_extractions"]
        
        accuracy = (successful / total_subjects * 100) if total_subjects > 0 else 0
        self.analysis["overall_accuracy"] = accuracy
        return accuracy
    
    def analyze_category_accuracy(self) -> Dict[str, float]:
        """åˆ†æå„é¡åˆ¥æº–ç¢ºç‡"""
        category_accuracy = {}
        
        for category_name, category_data in self.results["categories"].items():
            total = category_data["total_subjects"]
            successful = category_data["successful"]
            accuracy = (successful / total * 100) if total > 0 else 0
            category_accuracy[category_name] = accuracy
        
        self.analysis["category_accuracy"] = category_accuracy
        return category_accuracy
    
    def analyze_subject_quality(self) -> Dict[str, Dict[str, Any]]:
        """åˆ†æå„ç§‘ç›®è³ªé‡æŒ‡æ¨™"""
        subject_quality = {}
        
        for category_name, category_data in self.results["categories"].items():
            for subject_name, subject_data in category_data["subjects"].items():
                if subject_data["success"]:
                    quality_metrics = self._calculate_quality_metrics(subject_data)
                    subject_quality[f"{category_name}_{subject_name}"] = quality_metrics
        
        self.analysis["subject_accuracy"] = subject_quality
        return subject_quality
    
    def _calculate_quality_metrics(self, subject_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¨ˆç®—ç§‘ç›®è³ªé‡æŒ‡æ¨™"""
        metrics = {
            "extraction_success": True,
            "csv_generation_success": False,
            "script_generation_success": False,
            "answer_processing_success": False,
            "question_count": 0,
            "answer_count": 0,
            "quality_score": 0.0
        }
        
        # CSVç”ŸæˆæˆåŠŸæŒ‡æ¨™
        csv_files = subject_data.get("csv_files", [])
        if len(csv_files) >= 4:  # æ¨™æº–4å€‹CSVæª”æ¡ˆ
            metrics["csv_generation_success"] = True
        
        # è…³æœ¬ç”ŸæˆæˆåŠŸæŒ‡æ¨™
        if "script_file" in subject_data and subject_data["script_file"]:
            metrics["script_generation_success"] = True
        
        # ç­”æ¡ˆè™•ç†æˆåŠŸæŒ‡æ¨™
        answers_count = subject_data.get("answers_count", 0)
        if answers_count > 0:
            metrics["answer_processing_success"] = True
            metrics["answer_count"] = answers_count
        
        # é¡Œç›®æ•¸é‡
        questions_count = subject_data.get("questions_count", 0)
        metrics["question_count"] = questions_count
        
        # è¨ˆç®—è³ªé‡åˆ†æ•¸
        quality_score = 0
        if metrics["extraction_success"]:
            quality_score += 25
        if metrics["csv_generation_success"]:
            quality_score += 25
        if metrics["script_generation_success"]:
            quality_score += 25
        if metrics["answer_processing_success"]:
            quality_score += 25
        
        metrics["quality_score"] = quality_score
        return metrics
    
    def analyze_quality_metrics(self) -> Dict[str, Any]:
        """åˆ†ææ•´é«”è³ªé‡æŒ‡æ¨™"""
        quality_metrics = {
            "high_quality_subjects": 0,  # è³ªé‡åˆ†æ•¸ >= 75
            "medium_quality_subjects": 0,  # è³ªé‡åˆ†æ•¸ 50-74
            "low_quality_subjects": 0,  # è³ªé‡åˆ†æ•¸ < 50
            "total_questions_extracted": 0,
            "total_answers_processed": 0,
            "csv_generation_rate": 0.0,
            "script_generation_rate": 0.0,
            "answer_processing_rate": 0.0
        }
        
        total_subjects = 0
        csv_success_count = 0
        script_success_count = 0
        answer_success_count = 0
        
        for category_name, category_data in self.results["categories"].items():
            for subject_name, subject_data in category_data["subjects"].items():
                if subject_data["success"]:
                    total_subjects += 1
                    
                    # çµ±è¨ˆCSVç”Ÿæˆ
                    csv_files = subject_data.get("csv_files", [])
                    if len(csv_files) >= 4:
                        csv_success_count += 1
                    
                    # çµ±è¨ˆè…³æœ¬ç”Ÿæˆ
                    if "script_file" in subject_data and subject_data["script_file"]:
                        script_success_count += 1
                    
                    # çµ±è¨ˆç­”æ¡ˆè™•ç†
                    if subject_data.get("answers_count", 0) > 0:
                        answer_success_count += 1
                    
                    # çµ±è¨ˆé¡Œç›®æ•¸é‡
                    questions_count = subject_data.get("questions_count", 0)
                    quality_metrics["total_questions_extracted"] += questions_count
                    
                    # çµ±è¨ˆç­”æ¡ˆæ•¸é‡
                    answers_count = subject_data.get("answers_count", 0)
                    quality_metrics["total_answers_processed"] += answers_count
                    
                    # è³ªé‡åˆ†æ•¸åˆ†é¡
                    quality_score = self._calculate_quality_metrics(subject_data)["quality_score"]
                    if quality_score >= 75:
                        quality_metrics["high_quality_subjects"] += 1
                    elif quality_score >= 50:
                        quality_metrics["medium_quality_subjects"] += 1
                    else:
                        quality_metrics["low_quality_subjects"] += 1
        
        # è¨ˆç®—æ¯”ç‡
        if total_subjects > 0:
            quality_metrics["csv_generation_rate"] = (csv_success_count / total_subjects) * 100
            quality_metrics["script_generation_rate"] = (script_success_count / total_subjects) * 100
            quality_metrics["answer_processing_rate"] = (answer_success_count / total_subjects) * 100
        
        self.analysis["quality_metrics"] = quality_metrics
        return quality_metrics
    
    def generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼æ•´é«”æº–ç¢ºç‡
        overall_accuracy = self.analysis["overall_accuracy"]
        if overall_accuracy >= 95:
            recommendations.append("âœ… æ•´é«”æº–ç¢ºç‡å„ªç§€ (â‰¥95%)ï¼Œç³»çµ±è¡¨ç¾è‰¯å¥½")
        elif overall_accuracy >= 90:
            recommendations.append("âš ï¸ æ•´é«”æº–ç¢ºç‡è‰¯å¥½ (90-95%)ï¼Œå¯é€²ä¸€æ­¥å„ªåŒ–")
        else:
            recommendations.append("âŒ æ•´é«”æº–ç¢ºç‡éœ€è¦æ”¹é€² (<90%)ï¼Œå»ºè­°æª¢æŸ¥ç³»çµ±é…ç½®")
        
        # åŸºæ–¼è³ªé‡æŒ‡æ¨™
        quality_metrics = self.analysis["quality_metrics"]
        
        if quality_metrics["csv_generation_rate"] < 100:
            recommendations.append(f"ğŸ“Š CSVç”Ÿæˆç‡ {quality_metrics['csv_generation_rate']:.1f}%ï¼Œå»ºè­°æª¢æŸ¥CSVç”Ÿæˆé‚è¼¯")
        
        if quality_metrics["script_generation_rate"] < 100:
            recommendations.append(f"ğŸ“ è…³æœ¬ç”Ÿæˆç‡ {quality_metrics['script_generation_rate']:.1f}%ï¼Œå»ºè­°æª¢æŸ¥Google Apps Scriptç”Ÿæˆ")
        
        if quality_metrics["answer_processing_rate"] < 80:
            recommendations.append(f"ğŸ” ç­”æ¡ˆè™•ç†ç‡ {quality_metrics['answer_processing_rate']:.1f}%ï¼Œå»ºè­°æ”¹é€²ç­”æ¡ˆæå–ç®—æ³•")
        
        # åŸºæ–¼ç§‘ç›®è³ªé‡åˆ†å¸ƒ
        high_quality = quality_metrics["high_quality_subjects"]
        medium_quality = quality_metrics["medium_quality_subjects"]
        low_quality = quality_metrics["low_quality_subjects"]
        total = high_quality + medium_quality + low_quality
        
        if total > 0:
            high_quality_rate = (high_quality / total) * 100
            if high_quality_rate >= 80:
                recommendations.append("ğŸŒŸ é«˜è³ªé‡ç§‘ç›®æ¯”ä¾‹å„ªç§€ï¼Œç³»çµ±ç©©å®šæ€§è‰¯å¥½")
            elif high_quality_rate >= 60:
                recommendations.append("ğŸ“ˆ é«˜è³ªé‡ç§‘ç›®æ¯”ä¾‹è‰¯å¥½ï¼Œå¯é€²ä¸€æ­¥æå‡")
            else:
                recommendations.append("ğŸ”§ é«˜è³ªé‡ç§‘ç›®æ¯”ä¾‹åä½ï¼Œå»ºè­°å„ªåŒ–è™•ç†æµç¨‹")
        
        self.analysis["recommendations"] = recommendations
        return recommendations
    
    def generate_report(self) -> str:
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        report = "# è€ƒå¤é¡Œæå–ç³»çµ±æº–ç¢ºç‡åˆ†æå ±å‘Š\n\n"
        
        # æ•´é«”çµ±è¨ˆ
        report += "## æ•´é«”çµ±è¨ˆ\n\n"
        report += f"- **ç¸½ç§‘ç›®æ•¸**: {self.results['test_info']['total_subjects']}\n"
        report += f"- **æˆåŠŸæå–æ•¸**: {self.results['test_info']['successful_extractions']}\n"
        report += f"- **å¤±æ•—æå–æ•¸**: {self.results['test_info']['failed_extractions']}\n"
        report += f"- **æ•´é«”æº–ç¢ºç‡**: {self.analysis['overall_accuracy']:.1f}%\n\n"
        
        # å„é¡åˆ¥æº–ç¢ºç‡
        report += "## å„é¡åˆ¥æº–ç¢ºç‡\n\n"
        for category, accuracy in self.analysis["category_accuracy"].items():
            report += f"- **{category}**: {accuracy:.1f}%\n"
        report += "\n"
        
        # è³ªé‡æŒ‡æ¨™
        quality_metrics = self.analysis["quality_metrics"]
        report += "## è³ªé‡æŒ‡æ¨™åˆ†æ\n\n"
        report += f"- **é«˜è³ªé‡ç§‘ç›®æ•¸**: {quality_metrics['high_quality_subjects']}\n"
        report += f"- **ä¸­ç­‰è³ªé‡ç§‘ç›®æ•¸**: {quality_metrics['medium_quality_subjects']}\n"
        report += f"- **ä½è³ªé‡ç§‘ç›®æ•¸**: {quality_metrics['low_quality_subjects']}\n"
        report += f"- **ç¸½æå–é¡Œç›®æ•¸**: {quality_metrics['total_questions_extracted']}\n"
        report += f"- **ç¸½è™•ç†ç­”æ¡ˆæ•¸**: {quality_metrics['total_answers_processed']}\n"
        report += f"- **CSVç”Ÿæˆç‡**: {quality_metrics['csv_generation_rate']:.1f}%\n"
        report += f"- **è…³æœ¬ç”Ÿæˆç‡**: {quality_metrics['script_generation_rate']:.1f}%\n"
        report += f"- **ç­”æ¡ˆè™•ç†ç‡**: {quality_metrics['answer_processing_rate']:.1f}%\n\n"
        
        # æ”¹é€²å»ºè­°
        report += "## æ”¹é€²å»ºè­°\n\n"
        for recommendation in self.analysis["recommendations"]:
            report += f"- {recommendation}\n"
        report += "\n"
        
        # è©³ç´°ç§‘ç›®åˆ†æ
        report += "## è©³ç´°ç§‘ç›®åˆ†æ\n\n"
        report += "| ç§‘ç›®åç¨± | æº–ç¢ºç‡ | é¡Œç›®æ•¸ | ç­”æ¡ˆæ•¸ | è³ªé‡åˆ†æ•¸ | ç‹€æ…‹ |\n"
        report += "|---|---|---|---|---|---|\n"
        
        for subject_key, quality_metrics in self.analysis["subject_accuracy"].items():
            status = "âœ… å„ªç§€" if quality_metrics["quality_score"] >= 75 else "âš ï¸ è‰¯å¥½" if quality_metrics["quality_score"] >= 50 else "âŒ éœ€æ”¹é€²"
            report += f"| {subject_key} | {quality_metrics['quality_score']:.0f}% | {quality_metrics['question_count']} | {quality_metrics['answer_count']} | {quality_metrics['quality_score']:.0f} | {status} |\n"
        
        return report
    
    def save_analysis(self, output_file: str):
        """ä¿å­˜åˆ†æçµæœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis, f, ensure_ascii=False, indent=4)

def main():
    """ä¸»å‡½æ•¸"""
    results_file = "test_output/comprehensive_test/comprehensive_test_results.json"
    
    if not os.path.exists(results_file):
        print(f"çµæœæª”æ¡ˆä¸å­˜åœ¨: {results_file}")
        return
    
    print("=== é–‹å§‹æº–ç¢ºç‡åˆ†æ ===")
    
    analyzer = AccuracyAnalyzer(results_file)
    
    # åŸ·è¡Œåˆ†æ
    overall_accuracy = analyzer.analyze_overall_accuracy()
    category_accuracy = analyzer.analyze_category_accuracy()
    subject_quality = analyzer.analyze_subject_quality()
    quality_metrics = analyzer.analyze_quality_metrics()
    recommendations = analyzer.generate_recommendations()
    
    # ç”Ÿæˆå ±å‘Š
    report = analyzer.generate_report()
    
    # ä¿å­˜çµæœ
    output_dir = "test_output/accuracy_analysis"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜åˆ†æçµæœ
    analysis_file = os.path.join(output_dir, "accuracy_analysis.json")
    analyzer.save_analysis(analysis_file)
    print(f"åˆ†æçµæœå·²ä¿å­˜è‡³: {analysis_file}")
    
    # ä¿å­˜å ±å‘Š
    report_file = os.path.join(output_dir, "accuracy_analysis_report.md")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"åˆ†æå ±å‘Šå·²ä¿å­˜è‡³: {report_file}")
    
    # è¼¸å‡ºç¸½çµ
    print(f"\n=== æº–ç¢ºç‡åˆ†æå®Œæˆ ===")
    print(f"æ•´é«”æº–ç¢ºç‡: {overall_accuracy:.1f}%")
    print(f"é«˜è³ªé‡ç§‘ç›®: {quality_metrics['high_quality_subjects']}")
    print(f"ç¸½æå–é¡Œç›®: {quality_metrics['total_questions_extracted']}")
    print(f"ç¸½è™•ç†ç­”æ¡ˆ: {quality_metrics['total_answers_processed']}")

if __name__ == "__main__":
    main()
